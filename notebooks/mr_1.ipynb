{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54970533",
   "metadata": {},
   "source": [
    "# MonReader\n",
    "## Introduction\n",
    "**Background:**\n",
    "- Our company develops innovative Artificial Intelligence and Computer Vision solutions that revolutionize industries. Machines that can see: We pack our solutions in small yet intelligent devices that can be easily integrated to your existing data flow. Computer vision for everyone: Our devices can recognize faces, estimate age and gender, classify clothing types and colors, identify everyday objects and detect motion. Technical consultancy: We help you identify use cases of artificial intelligence and computer vision in your industry. Artificial intelligence is the technology of today, not the future.\n",
    "\n",
    "- MonReader is a new mobile document digitization experience for the blind, for researchers and for everyone else in need for fully automatic, highly fast and high-quality document scanning in bulk. It is composed of a mobile app and all the user needs to do is flip pages and everything is handled by MonReader: it detects page flips from low-resolution camera preview and takes a high-resolution picture of the document, recognizing its corners and crops it accordingly, and it dewarps the cropped document to obtain a bird's eye view, sharpens the contrast between the text and the background and finally recognizes the text with formatting kept intact, being further corrected by MonReader's ML powered redactor.\n",
    "\n",
    "**Data Description:**\n",
    "- We collected page flipping video from smart phones and labelled them as flipping and not flipping.\n",
    "- We clipped the videos as short videos and labelled them as flipping or not flipping. The extracted frames are then saved to disk in a sequential order with the following naming structure: VideoID_FrameNumber\n",
    "\n",
    "**Download Data:**\n",
    "- https://drive.google.com/file/d/1KDQBTbo5deKGCdVV_xIujscn5ImxW4dm/view?usp=sharing\n",
    "\n",
    "**Goal(s):**\n",
    "- Predict if the page is being flipped using a single image.\n",
    "\n",
    "**Success Metrics:**\n",
    "- Evaluate model performance based on F1 score, the higher the better.\n",
    "\n",
    "**Bonus(es):**\n",
    "- Predict if a given sequence of images contains an action of flipping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ee3654",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3fbaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warnings imports\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "# Pandas configuration\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"max_colwidth\", -1)\n",
    "\n",
    "# Basic imports\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import random\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "# Other imports\n",
    "import gradio as gr\n",
    "import PIL\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Tensorflow imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as bk\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.layers import Dense, Flatten\n",
    "\n",
    "# Pre-trained models\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB5\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeb4cba",
   "metadata": {},
   "source": [
    "### Helper function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78a309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints a horizontal line for delineating outputs\n",
    "def barrier():\n",
    "    print(\"\\n <<<\", \"-\" * 50, \">>> \\n\")\n",
    "\n",
    "# Saves models\n",
    "def save_model(model, name):\n",
    "    with open(\"models/\" + name + \".pkl\", \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"Model: {name}.pkl saved.\")\n",
    "\n",
    "# Loads models\n",
    "def load_model(name):\n",
    "    with open(\"models/\" + name, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    return model\n",
    "\n",
    "# Emboldening text\n",
    "def print_bold(text):\n",
    "    start = \"\\033[1m\"\n",
    "    end = \"\\033[0;0m\"\n",
    "    print(start + str(text) + end)\n",
    "\n",
    "# Emboldening text\n",
    "def bold(text):\n",
    "    start = \"\\033[1m\"\n",
    "    end = \"\\033[0;0m\"\n",
    "    return start + str(text) + end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764dbdbb-416f-4e1b-806c-aaba532f662c",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bfcdaa-f23a-4388-bdf4-3859adfe814f",
   "metadata": {},
   "source": [
    "### Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f79f0b8-2e9b-4a65-bdb5-7a40ebfae69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height, img_width, img_channels = 180, 180, 3\n",
    "batch_size = 32\n",
    "data_dir = \"../images/training\"\n",
    "test_data_dir = \"../images/testing\"\n",
    "loading_dataset_seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eee782-ef2a-4f77-8350-f510d5dda3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=loading_dataset_seed,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905c9df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=loading_dataset_seed,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f17d770-2ffc-414d-a62b-0d7b5daec8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    test_data_dir,\n",
    "    seed=loading_dataset_seed,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42096539-c970-42bf-a3bb-4b2e709893db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of training data\n",
    "for image_batch, labels_batch in train_ds.take(1):\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23139629-3da1-4a32-bd95-dcd6cac13453",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_names = train_ds.class_names\n",
    "print(classes_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88af9ae7-9815-4335-ba1b-8b1b0325cc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(6):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(classes_names[labels[i]])\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d4d4c4-0506-4d7f-84bb-609fcb592233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_data):\n",
    "    results = {}    \n",
    "    predicted_labels = []\n",
    "    true_labels = []\n",
    "    all_images = []\n",
    "    for images, labels in test_data:\n",
    "        true_labels.extend(labels.numpy())\n",
    "        predicted_labels.extend(tf.argmax(model.predict(images), axis=1).numpy())\n",
    "    # Accuracy\n",
    "    results[\"accuracy\"] = accuracy_score(true_labels, predicted_labels)\n",
    "    # F1 Score\n",
    "    results[\"f1_score\"] = f1_score(true_labels, predicted_labels)\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    class_names = test_data.class_names\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96de9ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = \n",
    "# test_data = \n",
    "\n",
    "# results = {}    \n",
    "# predicted_labels = []\n",
    "# true_labels = []\n",
    "# all_images = []\n",
    "\n",
    "# for images, labels in test_data:\n",
    "#     true_labels.extend(labels.numpy())\n",
    "#     predicted_labels.extend(tf.argmax(model.predict(images), axis=1).numpy())\n",
    "\n",
    "# # Accuracy\n",
    "# results[\"accuracy\"] = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "# # F1 Score\n",
    "# results[\"f1_score\"] = f1_score(true_labels, predicted_labels)\n",
    "# cm = confusion_matrix(true_labels, predicted_labels)\n",
    "# class_names = test_data.class_names\n",
    "# results\n",
    "\n",
    "# ######################################################################################################################\n",
    "\n",
    "# # plt.figure(figsize=(8, 6))\n",
    "# # sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "# # plt.xlabel(\"Predicted Labels\")\n",
    "# # plt.ylabel(\"True Labels\")\n",
    "# # plt.title(\"Confusion Matrix\")\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ec0349-80d6-4ea0-88f3-13b9788cbb73",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d16433b-caf1-4bc3-9c5a-a4c83df2359e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109be965-2cad-42d0-8fd6-3d237506b1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model = models.Sequential([\n",
    "    layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\", input_shape=(img_width, img_height, img_channels)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    # Dense Layers\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(len(classes_names), activation=\"softmax\")])\n",
    "\n",
    "custom_model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c134c6-01f1-4acd-855f-5cae0917cba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5177332-6080-4600-85d9-4cca73fcdd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model_history = custom_model.fit(train_ds, validation_data=val_ds, epochs=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2829114-b9ba-4e30-97c1-4534f6aca3bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Evaluating The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c987d9e-3fc6-4b6b-a031-be148ccd747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.gcf()\n",
    "plt.plot(custom_model_history.history[\"accuracy\"])\n",
    "plt.plot(custom_model_history.history[\"val_accuracy\"])\n",
    "plt.axis(ymin=0.4, ymax=1)\n",
    "plt.grid()\n",
    "plt.title(\"Custom Model Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend([\"train\", \"validation\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca5a337-4628-49c5-bd6e-53babb51379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(custom_model_history.history[\"loss\"])\n",
    "plt.plot(custom_model_history.history[\"val_loss\"])\n",
    "plt.grid()\n",
    "plt.title(\"Custom Model Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend([\"train\", \"validation\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844eccda-a0be-4201-bd39-96e3a51b9c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model_evaluation = evaluate_model(custom_model, test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b30a24-1847-4de4-bc17-25e456f64a26",
   "metadata": {},
   "source": [
    "#### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ac8b78-1a43-484f-b08d-f01006b3ad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model_path = \"../models/custom_model.h5\"\n",
    "custom_model.save(custom_model_path)\n",
    "custom_model_size = os.path.getsize(custom_model_path)/(1024*1024)\n",
    "print(f\"Model size: {custom_model_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03452b4b-fdc1-4329-9bea-d066ea5f7b5d",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb83277-7986-413e-8bc8-e366884262a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = Sequential()\n",
    "\n",
    "pretrained_model = tf.keras.applications.ResNet50(include_top=False, input_shape=(img_width, img_height, img_channels),\n",
    "                                                 pooling=\"avg\", classes=len(classes_names), weights=\"imagenet\")\n",
    "\n",
    "for layer in pretrained_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "resnet_model.add(pretrained_model)\n",
    "resnet_model.add(layers.Flatten())\n",
    "resnet_model.add(layers.Dense(512, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "resnet_model.add(layers.Dense(len(classes_names), activation=\"softmax\"))\n",
    "resnet_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=\"sparse_categorical_crossentropy\",\n",
    "                     metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f22320-bc76-42fd-96bd-95db52e6255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c3077c-6b3a-4da9-969c-fef272177751",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model_history = resnet_model.fit(train_ds, validation_data=val_ds, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aa06bd-c1dc-4c71-91c5-16137e9b4f4f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Evaluating The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b12714-ae65-4d6a-ada4-6cf2ab0af186",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.gcf()\n",
    "plt.plot(resnet_model_history.history[\"accuracy\"])\n",
    "plt.plot(resnet_model_history.history[\"val_accuracy\"])\n",
    "plt.axis(ymin=0.4, ymax=1)\n",
    "plt.grid()\n",
    "plt.title(\"ResNet Model Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend([\"train\", \"validation\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f409be3-8449-4c95-8a40-0a97829b8c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(resnet_model_history.history[\"loss\"])\n",
    "plt.plot(resnet_model_history.history[\"val_loss\"])\n",
    "plt.grid()\n",
    "plt.title(\"ResNet Model Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend([\"train\", \"validation\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36194599-a7ff-4c5f-a75a-2691c407e299",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model_evaluation = evaluate_model(resnet_model, test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6baab0-4239-477d-bfca-040b8f8194a0",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7547577c-1e30-4a97-9c03-bfb6af13a541",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model_path = \"../models/resnet_model.weights.h5\"\n",
    "resnet_model.save_weights(resnet_model_path)\n",
    "\n",
    "resnet_model_size = os.path.getsize(resnet_model_path)/(1024*1024)\n",
    "print(f\"Model size: {resnet_model_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35436788-4115-4f31-ba46-041eee234b9b",
   "metadata": {},
   "source": [
    "### MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2a9f4e-1457-42a0-8010-403594ec1552",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_model = Sequential()\n",
    "\n",
    "mobilenet_pretrained_model= tf.keras.applications.mobilenet_v2.MobileNetV2(\n",
    "    include_top=False,\n",
    "    input_shape=(img_width, img_height, img_channels),\n",
    "    pooling=\"avg\",\n",
    "    weights=\"imagenet\")\n",
    "\n",
    "for layer in mobilenet_pretrained_model.layers:\n",
    "    layer.trainable=False\n",
    "\n",
    "mobilenet_model.add(mobilenet_pretrained_model)\n",
    "mobilenet_model.add(layers.Flatten())\n",
    "mobilenet_model.add(layers.Dense(512, activation=\"relu\"))\n",
    "mobilenet_model.add(layers.Dense(len(classes_names), activation=\"softmax\"))\n",
    "\n",
    "mobilenet_model.compile(optimizer=tf.keras.optimizers.Adam(), loss=\"sparse_categorical_crossentropy\",\n",
    "                        metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d349ec-3ab3-4e39-9954-3a0eb2d2857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72849bef-2e2f-4b33-a75c-065805edbb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_model_history = mobilenet_model.fit(train_ds, validation_data=val_ds, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddecd55-bf34-4813-b390-09aa12e29277",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Evaluating The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c07a581-78ba-4761-928a-4efa631e83f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.gcf()\n",
    "plt.plot(mobilenet_model_history.history[\"accuracy\"])\n",
    "plt.plot(mobilenet_model_history.history[\"val_accuracy\"])\n",
    "plt.axis(ymin=0.4,ymax=1)\n",
    "plt.grid()\n",
    "plt.title(\"MobileNet Model Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend([\"train\", \"validation\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d47905d-f434-469d-b5bf-cd96a1f0dd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mobilenet_model_history.history[\"loss\"])\n",
    "plt.plot(mobilenet_model_history.history[\"val_loss\"])\n",
    "plt.grid()\n",
    "plt.title(\"MobileNet Model Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend([\"train\", \"validation\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5d3bc9-f6ba-448d-a9d2-05f3a7278b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_model_evaluation = evaluate_model(mobilenet_model, test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01daf0e-f724-44b9-8f94-00fb0420fb38",
   "metadata": {},
   "source": [
    "#### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65c772d-39e4-4216-9fbf-2027ac72f45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_model_path = \"../models/mobilenet_model.weights.h5\"\n",
    "mobilenet_model.save_weights(mobilenet_model_path)\n",
    "mobilenet_model_size = os.path.getsize(mobilenet_model_path)/(1024*1024)\n",
    "print(f\"Model size: {mobilenet_model_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0219cd76-a353-4bdd-af23-28060c4304ec",
   "metadata": {},
   "source": [
    "### Final model collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0429ef-a83d-4188-8093-2fdcf2196304",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = {\"accuracy\":[custom_model_evaluation[\"accuracy\"],\n",
    "                      resnet_model_evaluation[\"accuracy\"],\n",
    "                      mobilenet_model_evaluation[\"accuracy\"]],\n",
    "          \"f1_score\":[custom_model_evaluation[\"f1_score\"],\n",
    "                      resnet_model_evaluation[\"f1_score\"],\n",
    "                      mobilenet_model_evaluation[\"f1_score\"]],\n",
    "          \"model size\":[np.round(custom_model_size, 2),\n",
    "                        np.round(resnet_model_size, 2),\n",
    "                        np.round(mobilenet_model_size, 2)]}\n",
    "\n",
    "df = pd.DataFrame(values, index=[\"custom_model\", \"resnet\", \"mobilenet\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17910858-6870-443d-b76d-5e5d2784adef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "328.167px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
